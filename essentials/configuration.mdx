---
title: 'Configuration'
description: 'How to Configure the Gateway'
---

Below details how to configure gateway. For more information on router types see the router page.

## Basic Configuration

At its simplest the gateway can be configured with a single router and two models.

This example shows a failover router leveraging OpenAI and Azure OpenAI. For OpenAI, Cohere, and OctoML, only an API key needs to be provided.
All other parameters, including the model, are provided by default. For Azure OpenAI, a model and base_url must be specified.

- `language` - This is the language API endpoint which supporta `/chat` endpoints from model providers
- `id` - A unique id for your router
- `strategy` - The type of router can be round-robin, weighted-round-robin, least-latency, priority
- `models/id` - **TODO: what values can this be?** A unique ID for the model configuration

```yaml
routers:
  language:
    - id: my-chat-router
      strategy: priority
      models:
        - id: primary
          openai:
            api_key: "XXXXXXXXX"
        - id: secondary
          azureopenai:
            api_key: "XXXXXX"
            model: "glide-GPT-35" # the Azure OpenAI deployment name
            base_url: "https://mydeployment.openai.azure.com/" # the name of your Azure OpenAI Resource
```

## Advanced Configuration

Additional, parameters can be specified in the configuration as well. Refer to the `Providers` page for more information on paramters.

```yaml
telemetry:
  logging:
    level: debug  # debug, info, warn, error, fatal
    encoding: console

api:
  http:
    listen_addr: 0.0.0.0:7685
    max_body_size: "2Mi"
    tls:
      ca_path:
      cert_path:
    # other configs

routers:
  language:
    - id: openai-pool
      strategy: priority # round-robin, weighted-round-robin, priority, least-latency, priority, etc.
      models:
        - id: primary
          openai: # cohere, azureopenai, gemini, other providers we support
            model: gpt-3.5-turbo
            api_key: ${env:OPENAI_API_KEY}
            default_params:
              temperature: 0.1

        - id: secondary
          cohere:
            model: command-light
            apiKey: ${env:COHERE_API_KEY}
            default_params: # set the default request params
              temperature: 0.1

    - id: latency-critical-pool
      strategy: least_latency
      models:
        - id: primary
          timeout_ms: 200
          openai:
            model: gpt-3.5-turbo
            api_key: ${env:OPENAI_API_KEY}
        
        - id: secondary
          timeout_ms: 200
          cohere:
            api_key: ${env:COHERE_API_KEY}

    - id: cohere-openai-ab-test
      strategy: weighted_round_robin
      models:
        - id: openai
          weight: 30
          openai:
            api_key: ${env:OPENAI_API_KEY}
        - id: cohere
          weight: 70
          cohere:
            api_key: ${env:COHERE_API_KEY}

    ```